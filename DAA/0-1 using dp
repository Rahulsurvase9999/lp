0/1 Knapsack Problem using Dyanamic Programming
Theory:
Branch and bound is an algorithm design paradigm which is generally used for solving
combinatorial optimization problems. These problems typically exponential in terms of time
complexity and may require exploring all possible permutations in worst case.
Let us consider below 0/1 Knapsack problem to understand Branch and Bound. Given two integer
arrays val[0..n-1] and wt[0..n-1] that represent values and weights associated with n items
respectively.
Find out the maximum value subset of val [] such that sum of the weights of this subset is smaller
than or equal to Knapsack capacity W. Let us explore all approaches for this problem.
1. A Greedy approach is to pick the items in decreasing order of value per unit weight. The
Greedy approach works only for fractional knapsack problem and may not produce correct
result for 0/1 knapsack.
2. We can use Dynamic Programming (DP) for 0/1 Knapsack problem. In DP, we use a 2D table
of size n x W. The DP Solution doesn’t work if item weights are not integers.
3. Since DP solution doesn’t always work, a solution is to use Brute Force. With n items, there
are 2n
solutions to be generated, check each to see if they satisfy the constraint, save maximum
solution that satisfies constraint. This solution can be expressed as tree
# This is the memoization approach of 
# 0 / 1 Knapsack in Python in simple 
# we can say recursion + memoization = DP 


def knapsack(wt, val, W, n): 

	# base conditions 
	if n == 0 or W == 0: 
		return 0
	if t[n][W] != -1: 
		return t[n][W] 

	# choice diagram code 
	if wt[n-1] <= W: 
		t[n][W] = max( 
			val[n-1] + knapsack( 
				wt, val, W-wt[n-1], n-1), 
			knapsack(wt, val, W, n-1)) 
		return t[n][W] 
	elif wt[n-1] > W: 
		t[n][W] = knapsack(wt, val, W, n-1) 
		return t[n][W] 

# Driver code 
if __name__ == '__main__': 
	profit = [60, 100, 120] 
	weight = [10, 20, 30] 
	W = 50
	n = len(profit) 
# We initialize the matrix with -1 at first. 
	t = [[-1 for i in range(W + 1)] for j in range(n + 1)] 
	print(knapsack(weight, profit, W, n)) 

Another code

# Function to solve the 0-1 Knapsack Problem using dynamic programming
def knapsack_01(values, weights, capacity):
    n = len(values)  # Number of items available for the knapsack problem

    # Initialize a table to store the maximum value for each (item, capacity) combination
    # dp[i][w] represents the maximum value that can be obtained with the first 'i' items
    # and a knapsack capacity of 'w'.
    dp = [[0 for _ in range(capacity + 1)] for _ in range(n + 1)]

    # Populate the table using dynamic programming
    for i in range(n + 1):  # Iterate through the available items
        for w in range(capacity + 1):  # Iterate through knapsack capacities from 0 to the given capacity

            # Base case 1: If there are no items (i == 0) or no remaining capacity (w == 0),
            # the maximum value is 0 because there are no items to include or no capacity to use.
            if i == 0 or w == 0:
                dp[i][w] = 0

            # Case 2: If the weight of the current item (weights[i-1]) is less than or equal to
            # the remaining capacity (w), we have a choice:
            # 1. Include the current item and add its value (values[i-1]) to the maximum value obtained with
            #    the remaining capacity (dp[i-1][w-weights[i-1]]).
            # 2. Exclude the current item and retain the maximum value obtained with the same capacity (dp[i-1][w]).
            elif weights[i - 1] <= w:
                dp[i][w] = max(values[i - 1] + dp[i - 1][w - weights[i - 1]], dp[i - 1][w])

            # Case 3: If the weight of the current item (weights[i-1]) is greater than the remaining capacity (w),
            # we cannot include the item, so we retain the maximum value obtained with the same capacity (dp[i-1][w]).
            else:
                dp[i][w] = dp[i - 1][w]

    # The final value in dp[n][capacity] represents the maximum value that can be obtained with all items
    # and the given capacity.

    # Find the items included in the optimal solution
    selected_items = []
    i, w = n, capacity
    while i > 0 and w > 0:
        if dp[i][w] != dp[i - 1][w]:
            selected_items.append(i - 1)
            w -= weights[i - 1]
        i -= 1

    selected_items.reverse()

    # Return the maximum value and a list of indices of selected items
    return dp[n][capacity], selected_items

# Example usage
if __name__ == "__main__":
    values = [10, 40, 30, 50]
    weights = [5, 4, 6, 3]
    capacity = 10

    max_value, selected_items = knapsack_01(values, weights, capacity)

    print(f"Maximum value in the knapsack: {max_value}")
    print("Selected items:")
    for i in selected_items:
        print(f"Item {i + 1} (Value: {values[i]}, Weight: {weights[i]})")

# Time Complexity Analysis:
# - Filling the dp table: O(n*capacity), where n is the number of items and capacity is the knapsack capacity.

# Space Complexity Analysis:
# - Space required for the dp table: O(n*capacity), where n is the number of items and capacity is the knapsack capacity.

"""
The 0/1 Knapsack Problem is a classic optimization problem in computer science and mathematics. It is a problem of combinatorial optimization where the goal is to select a combination of items, each with a given weight and value, to maximize the total value while not exceeding a given weight limit (the capacity of the knapsack). The term "0/1" indicates that for each item, you can either choose to include it (1) or exclude it (0) in the knapsack, meaning you can't take a fractional part of an item. In other words, it's a binary choice for each item.

Here's a formal description of the problem:

- You have a set of n items, each with a specific weight (w_i) and value (v_i).
- You have a knapsack with a maximum weight capacity (W).
- You can choose to include or exclude each item in the knapsack (0 or 1).
- The goal is to find the combination of items to include in the knapsack, such that the total weight of the selected items does not exceed W, and the total value is maximized.

Mathematically, the problem can be defined as follows:

Maximize Σ(v_i * x_i) for i = 1 to n
Subject to Σ(w_i * x_i) for i = 1 to n ≤ W
x_i = 0 or 1 for all i

Where:
- v_i is the value of item i.
- w_i is the weight of item i.
- x_i is a binary variable (0 or 1) indicating whether item i is included (1) or excluded (0) from the knapsack.
- W is the maximum weight capacity of the knapsack.

The 0/1 Knapsack Problem is known to be an NP-hard problem, meaning there is no known algorithm to solve it optimally in polynomial time for all inputs. However, dynamic programming and branch and bound algorithms can be used to find the optimal solution for relatively small instances of the problem. For larger instances, heuristics and approximation algorithms are often used to find near-optimal solutions.

This problem has various practical applications, such as resource allocation, portfolio optimization, and cutting stock problems.

-----------------------------

Dynamic programming is a problem-solving technique in computer science and mathematics used to efficiently solve problems by breaking them down into smaller overlapping subproblems and storing the results of these subproblems to avoid redundant computations. It is a powerful approach for solving optimization problems, particularly those with recursive or overlapping substructures. Here are the key principles and concepts of dynamic programming:

1. **Optimal Substructure:** Dynamic programming problems can be broken down into subproblems, and the optimal solution to the original problem can be constructed from optimal solutions to its subproblems. This is often expressed as a recursive formula or relation.

2. **Overlapping Subproblems:** In many dynamic programming problems, the same subproblems are encountered multiple times during the computation. Dynamic programming avoids redundant work by storing the results of already-solved subproblems in a data structure (usually a table or array) for later use.

3. **Memoization:** Memoization is the process of storing intermediate results (such as function values) in a data structure (often an array or dictionary) and returning the cached result when the same inputs reappear. This avoids redundant computation in recursive algorithms.

4. **Bottom-Up Approach:** Dynamic programming can be solved using either a top-down approach (recursion with memoization) or a bottom-up approach (iterative approach). In the bottom-up approach, you start by solving the smallest subproblems and build up to the original problem iteratively.

5. **State Transition:** Dynamic programming problems involve transitions from one state to another. These transitions can be represented by a recurrence relation or formula. The relation between subproblems and the order in which they are solved are crucial for finding an efficient solution.

6. **Tabulation:** In the bottom-up approach, you use a table (usually a multi-dimensional array) to store the results of subproblems. Each entry in the table corresponds to a specific state, and you fill it based on the optimal solution to smaller subproblems.

7. **Examples:** Dynamic programming is used in various problem domains, including:
   - **Fibonacci Sequence:** Calculating Fibonacci numbers using dynamic programming to avoid redundant calculations.
   - **0/1 Knapsack Problem:** Finding the most valuable combination of items to fit in a knapsack with limited capacity.
   - **Shortest Path Problems:** Finding the shortest path between nodes in a graph (e.g., Dijkstra's algorithm).
   - **Sequence Alignment:** Aligning sequences, such as DNA or protein sequences, to find the best match.
   - **Dynamic Time Warping:** Measuring the similarity between two time series sequences.
   - **Longest Common Subsequence (LCS):** Finding the longest subsequence common to two sequences.

8. **Time and Space Complexity:** The time and space complexity of dynamic programming solutions can vary depending on the specific problem and the chosen approach. While it often provides efficient solutions, dynamic programming can be computationally expensive for problems with large input sizes. Memoization and tabulation help manage time and space complexity.

Overall, dynamic programming is a versatile technique for solving a wide range of optimization problems, and its efficiency depends on how well the problem can be divided into smaller subproblems and how effectively overlapping subproblems are handled.
"""


